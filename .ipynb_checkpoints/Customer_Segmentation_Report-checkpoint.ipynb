{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c78c83-1aec-4d06-b80b-2ab703343c27",
   "metadata": {},
   "source": [
    "# Customer Segmentation Report\n",
    "Customer segmentation is a way to identify groups of similar customers. Customers can be segmented on a wide variety of characteristics, such as demographic information, purchase behavior, and attitudes. This template provides an end-to-end report for processing and segmenting customer purchase data using a K-means clustering algorithm. It also includes a snake plot and heatmap to visualize the resulting clusters and feature importance.\n",
    "\n",
    "The dataset used consists of customer data, including purchase recency, frequency, and monetary value where each row represents a different customer with a distinct customer ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf21890",
   "metadata": {},
   "source": [
    "## Imports and Data Preparation\n",
    "The code below imports the packages necessary for data manipulation, visualization, pre-processing, and clustering. It also sets up the visualization style and loads in the data.\n",
    "\n",
    "Finally, it inspects the data types and missing values with the [`.info()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) method from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336d7477-fda2-4903-9ec9-ef2f75349094",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Brian/Documents/GitHub/Marketing-Analytics-Projects/data/customer_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15876\\2078862579.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Load the data and replace with your CSV file path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/Users/Brian/Documents/GitHub/Marketing-Analytics-Projects/data/customer_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Preview the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Brian/Documents/GitHub/Marketing-Analytics-Projects/data/customer_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Load the data and replace with your CSV file path\n",
    "df = pd.read_csv(\"/data/customer_data.csv\")\n",
    "\n",
    "# Preview the data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2101794-8fa6-48d7-bc30-33a2cbfb303d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check columns for data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e70c71",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Based on the evaluation above, we select columns that we wish to inspect further. Three columns are selected from the four columns. CustomerID is omitted because it is an identifier and not useful for clustering.\n",
    "\n",
    "Printing descriptive statistics is helpful because K-means clustering has several key assumptions that can be revealed via this exploration:\n",
    "1. There is no skewness to the data.\n",
    "2. The variables have the same average values.\n",
    "3. The variables have the same variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for clustering\n",
    "columns_for_clustering = [\"Recency\", \"Frequency\", \"MonetaryValue\"]\n",
    "\n",
    "# Create new DataFrame with clustering variables\n",
    "df_features = df[columns_for_clustering]\n",
    "\n",
    "# Print a summary of descriptive statistics\n",
    "df_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-psychiatry",
   "metadata": {},
   "source": [
    "The [`facetgrid()`](https://seaborn.pydata.org/generated/seaborn.FacetGrid.html) function from `seaborn` creates a grid of histograms of the data to be clustered.  It serves as a further exploration of the data to determine its skew and whether it needs transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b6d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distributions of the selected variables\n",
    "g = sns.FacetGrid(\n",
    "    df_features.melt(),  # Reformat the DataFrame for plotting purposes\n",
    "    col=\"variable\",  # Split on the 'variable' column created by reformating\n",
    "    sharey=False,  # Turn off shared y-axis\n",
    "    sharex=False,  # Turn off shared x-axis\n",
    ")\n",
    "# Apply a histogram to the facet grid\n",
    "g.map(sns.histplot, \"value\")\n",
    "# Adjust the top of the plots to make room for the title\n",
    "g.fig.subplots_adjust(top=0.8)\n",
    "# Create a title\n",
    "g.fig.suptitle(\"Unprocessed Variable Distributions\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ea05a",
   "metadata": {},
   "source": [
    "Before proceeding, it is crucial to ensure that all columns selected for clustering are numeric. The following code iterates through the reduced DataFrame and checks whether each column is numeric. If it returns `True`, then you can proceed with the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a16dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all([pd.api.types.is_numeric_dtype(df_features[col]) for col in columns_for_clustering])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bad38a",
   "metadata": {},
   "source": [
    "## Data Pre-processing \n",
    "Based on the grids above, if there is a skew, we will have to remove the skew and center the variables. This is the case for the dataset used.\n",
    "\n",
    "We apply a log transformation to the data and scale the data using `StandardScaler()`, centering and scaling the data in further preparation for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a log transformation of the data to unskew the data\n",
    "df_log = np.log(df_features)\n",
    "\n",
    "# Initialize a standard scaler and fit it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_log)\n",
    "\n",
    "# Scale and center the data\n",
    "df_normalized = scaler.transform(df_log)\n",
    "\n",
    "# Create a pandas DataFrame of the processed data\n",
    "df_processed = pd.DataFrame(\n",
    "    data=df_normalized, index=df_features.index, columns=df_features.columns\n",
    ")\n",
    "\n",
    "# Plot the distributions of the selected variables\n",
    "g = sns.FacetGrid(df_processed.melt(), col=\"variable\")\n",
    "g.map(sns.histplot, \"value\")\n",
    "g.fig.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle(\"Preprocessed Variable Distributions\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5abde3-eda6-40b5-bde7-25a6477373da",
   "metadata": {},
   "source": [
    "## Cluster Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca1908-736b-4f0b-9966-a90118863279",
   "metadata": {},
   "source": [
    "Our next step is to fit a variable number of clusters and plot each cluster's sum-of-squared errors (SSE). The SSE reflects the sum of squared distances from every data point to the cluster center. The aim is to reduce the SSE while still maintaining a reasonable number of clusters. \n",
    "\n",
    "By plotting the SSE for each number of clusters, we can identify at what point there are diminishing returns by adding new clusters, using an elbow plot.\n",
    "\n",
    "In the code below, we can set the maximum number of clusters we want to plot, and then a loop is used to generate the SSE for each number of clusters. Finally, the `seaborn` function `pointplot()` plots a curve with each cluster number and SSE. This allows us to identify the 'elbow' or point where there are only marginal reductions for each additional cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af5116-21c3-43cb-aa82-9febfd29dec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the maximum number of clusters to plot\n",
    "max_clusters = 10\n",
    "\n",
    "# Initialize empty dictionary to store sum of squared errors\n",
    "sse = {}\n",
    "\n",
    "# Fit KMeans and calculate SSE for each k\n",
    "for k in range(1, max_clusters):\n",
    "    # Initialize KMeans with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "    # Fit KMeans on the normalized dataset\n",
    "    kmeans.fit(df_processed)\n",
    "    # Assign sum of squared distances to k element of dictionary\n",
    "    sse[k] = kmeans.inertia_\n",
    "\n",
    "# Initialize a figure of set size\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Create an elbow plot of SSE values for each key in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "\n",
    "# Add labels to the plot\n",
    "plt.title(\"Elbow Method Plot\", fontsize=16)  # Add a title to the plot\n",
    "plt.xlabel(\"Number of Clusters\")  # Add x-axis label\n",
    "plt.ylabel(\"SSE\")  # Add y-axis label\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c8d6a",
   "metadata": {},
   "source": [
    "We can now select an optimal number of clusters based on the elbow plot above by setting `k`. In this example, `k` is set to 3.\n",
    "\n",
    "`KMeans()` from `sklearn.cluster` with `k` clusters is then fit to the processed data, and the cluster labels are extracted and assigned back to the original data. This allows us to inspect raw data by cluster in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of clusters\n",
    "k = 3\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=k, random_state=1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(df_processed)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Create a new DataFrame by adding a new cluster column to the original data\n",
    "df_clustered = df.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Preview the clustered DataFrame\n",
    "df_clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0f9a2",
   "metadata": {},
   "source": [
    "## Model  Interpretation/Visualization\n",
    "### Visualizing the Raw Values by Cluster\n",
    "The next step is to analyze the unprocessed data by cluster. The `pandas` method [`DataFrame.groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html), combined with the [`.size()`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.size.html) method, returns the total number of rows per `Cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5030df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by cluster and calculate the total number of rows per group\n",
    "df_sizes = df_clustered.groupby([\"Cluster\"], as_index=False).size()\n",
    "\n",
    "# Inspect the row counts\n",
    "df_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cdd263",
   "metadata": {},
   "source": [
    "Next, the mean values per cluster are visualized. The data is grouped again, and this time, the `pandas` method [`.mean()`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.mean.html) is used to aggregate the data by cluster and calculate the mean for each variable. Alternatively, the [`.agg()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html) method can also be used to specify specific aggregations for different columns if necessary. Consult the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html) for further information on the types of aggregations possible.\n",
    "\n",
    "The `seaborn` [`catplot()`](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot) function visualizes the means per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2675e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the mean of feature columns by cluster\n",
    "df_means = df_clustered.groupby([\"Cluster\"])[df_features.columns].mean().reset_index()\n",
    "\n",
    "# Plot the distributions of the selected variables\n",
    "sns.catplot(\n",
    "    data=df_means.melt(id_vars=\"Cluster\"),  # Transform the data to enable plotting\n",
    "    col=\"variable\",\n",
    "    x=\"Cluster\",\n",
    "    y=\"value\",\n",
    "    kind=\"bar\",\n",
    ")\n",
    "\n",
    "# Add a title\n",
    "plt.suptitle(\"Average Values by Cluster\", y=1.04, fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ea070",
   "metadata": {},
   "source": [
    "### Snake Plot of the Clusters\n",
    "The next step takes the processed data and visualizes the differences between the clusters using a snake plot. This can be helpful spot trends or key differences that would not be visible with the raw data. The code below uses the following code:\n",
    "- [`DataFrame.assign()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html) adds the cluster labels to the processed data.\n",
    "- [`DataFrame.melt()`](https://pandas.pydata.org/pandas-docs/version/1.0.0/reference/api/pandas.DataFrame.melt.html) transforms the data from wide to long format, which makes plotting easier.\n",
    "- Finally, the `seaborn` [`lineplot()`](https://seaborn.pydata.org/generated/seaborn.lineplot.html) function is used to plot three lines, one for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98cba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cluster labels to processed DataFrame\n",
    "df_processed_clustered = df_processed.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Melt the normalized DataFrame and reset the index\n",
    "df_processed_melt = pd.melt(\n",
    "    df_processed_clustered.reset_index(),\n",
    "    # Assign the cluster labelss as the ID\n",
    "    id_vars=['Cluster'],\n",
    "    # Assign clustering variables as values\n",
    "    value_vars=df_features.columns,\n",
    "    # Name the variable and value\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Value\",\n",
    ")\n",
    "\n",
    "# Change the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Add label and titles to the plot\n",
    "plt.title('Snake Plot of Normalized Variables', fontsize=16)\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Average Normalized Value')\n",
    "\n",
    "# Plot a line for each value of the cluster variable\n",
    "sns.lineplot(data=df_processed_melt, x='Metric', y='Value', hue='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe87e0a",
   "metadata": {},
   "source": [
    "### Heatmap of Relative Importances\n",
    "Another technique to help visualize how each segment is distinct is to plot the relative importance. The code below achieves this by doing the following:\n",
    "- First, it calculates the average values for each cluster.\n",
    "- Next, it calculates the average values for the total population.\n",
    "- It then divides the cluster averages by the population averages and subtracts one.\n",
    "\n",
    "This provides a relative importance score for each of the different features used for clustering. The `seaborn` [`heatmap()`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) function plots these relative importances on a red to blue color scale to help visualize the relative importance of each attribute to the segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72196b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate average RFM values for each cluster\n",
    "cluster_avg = df_clustered.groupby([\"Cluster\"])[columns_for_clustering].mean()\n",
    "\n",
    "# Calculate average RFM values for the total customer population\n",
    "population_avg = df[columns_for_clustering].mean()\n",
    "\n",
    "# Calculate relative importance of cluster's attribute value compared to population\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "# Change the figure size\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Add the plot title\n",
    "plt.title(\"Relative importance of attributes\", fontsize=16)\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt=\".2f\", cmap=sns.diverging_palette(20, 220, as_cmap=True))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
